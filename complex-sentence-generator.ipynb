{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Introductory Examples for the NLTK Book ***\n",
      "Loading text1, ..., text9 and sent1, ..., sent9\n",
      "Type the name of the text or sentence to view it.\n",
      "Type: 'texts()' or 'sents()' to list the materials.\n",
      "text1: Moby Dick by Herman Melville 1851\n",
      "text2: Sense and Sensibility by Jane Austen 1811\n",
      "text3: The Book of Genesis\n",
      "text4: Inaugural Address Corpus\n",
      "text5: Chat Corpus\n",
      "text6: Monty Python and the Holy Grail\n",
      "text7: Wall Street Journal\n",
      "text8: Personals Corpus\n",
      "text9: The Man Who Was Thursday by G . K . Chesterton 1908\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.collocations import *\n",
    "from nltk.book import *\n",
    "from random import randrange #, uniform\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = text1\n",
    "vocab = set(text1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_text = [text1, text2, text3, text4, text5, text6, text7, text8, text9]\n",
    "bigramy = []\n",
    "for t1 in list_text:\n",
    "    k = [x.lower() for x in t1.tokens if x.isalpha()]\n",
    "#     bigramy += sorted(list(nltk.bigrams(k)))    \n",
    "    bigramy += sorted(set(list(nltk.bigrams(k))))\n",
    "\n",
    "\n",
    "trigramy=[]\n",
    "for t1 in list_text:\n",
    "    k = [x.lower() for x in t1.tokens if x.isalpha()]\n",
    "    trigramy += sorted(set(list(nltk.trigrams(k))))\n",
    "#     trigramy += sorted(list(nltk.trigrams(k)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Różne 363092\n",
      "[((',\"', 'said', 'the'), 113), (('.', '\"', 'I'), 104), ((',\"', 'said', 'Syme'), 95), ((',\"', 'he', 'said'), 76), (('Dr', '.', 'Bull'), 70), (('he', 'said', ','), 48), (('.', '\"', 'You'), 47), (('don', \"'\", 't'), 47), (('said', 'Syme', ','), 44), (('Syme', '.', '\"'), 43)]\n"
     ]
    }
   ],
   "source": [
    "print(\"Różne \"+ str(len(bigramy)))\n",
    "\n",
    "ziarno = \"man\"\n",
    "for w1, w2, w3 in trigramy:\n",
    "    if w1 == ziarno:\n",
    "#         print(w1, w2, w3)\n",
    "        pass\n",
    "\n",
    "f1 = FreqDist(list(nltk.bigrams(t1)))\n",
    "f2 = FreqDist(list(nltk.trigrams(t1)))\n",
    "print(f2.most_common(10))\n",
    "for w in f1:\n",
    "    if w[0]==\"man\" and w[1].isalpha():\n",
    "#         print(w)\n",
    "        pass\n",
    "\n",
    "# print(f1.most_common(10))\n",
    "# for i in f1.most_common():\n",
    "#     if i[0][0]=='man':print(i[0][1])\n",
    "\n",
    "# for i in f1:\n",
    "#     if i[0]=='man': print(i[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "word = \"sunny\"\n",
    "def get_all_word_synonyms(word):\n",
    "    word = word.lower()\n",
    "    synonyms = []\n",
    "    synsets = wordnet.synsets(word)\n",
    "    if (len(synsets) == 0):\n",
    "        return []\n",
    "    for synset in synsets:\n",
    "        lemma_names = synset.lemma_names()\n",
    "        for lemma_name in lemma_names:\n",
    "            lemma_name = lemma_name.lower().replace('_', ' ')\n",
    "            if (lemma_name != word and lemma_name not in synonyms):\n",
    "                synonyms.append(lemma_name)\n",
    "    return synonyms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 464,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'DT'"
      ]
     },
     "execution_count": 464,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "s = ['the']\n",
    "\n",
    "nltk.pos_tag(s)[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Podaj przybliżoną liczbę słów generowanego tekstu: \n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " 500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Podaj ziarno do generowania tekstu: \n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " dog\n"
     ]
    }
   ],
   "source": [
    "print(\"Podaj przybliżoną liczbę słów generowanego tekstu: \")\n",
    "max_slow = input()\n",
    "int(max_slow)\n",
    "print(\"Podaj ziarno do generowania tekstu: \")\n",
    "ziarno = input()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Man\n",
      "Man Who Was Thursday by thousand things instead of firing pistols in this room with a man. Who shall find him all the man I tell. You in front of which he cried out over the Professor worn them too closely packed. Or saints your religion involves that I am going to have a little back. Garden empty rooms and it is to me in the President Sunday School delicacy of which he could not. I do all the world is full of shattered sunlight and said. In front I am a little. I am a police or any rate of racing it fell. A sort of doubtful salute with the other hand off the end of his own white feather as I am I do. Not serious anarchist I am not in the Marquis is. I am going to tell you think of his. Own safety and sociability of the man. In a man of it is the other hand on I say. That it was too shocking to him like the Marquis sprang. Back of a man dressed as he spoke earnestly of the old. I am not a matter of the world is full. I father cultivated art and the end I am afraid it is. A little I can see you will not think that the man was a pair of his own. Safety for me to tell you that I am not a voice. I can behave like any common bodies of the world a good mood for. Delicacies of his own as much the same boat before. Conversation flowed I can see that I am a man who had become inured to him like. The Professor worn them that bishops in our little. Line as if the Professor made an effort. To me the cry of a pair of Saffron Park I am not serious. This man with I am a little gardens. Were four hours of him again opened his. Forehead and they could not be too. Great cloak down and it is the only evening of a man who had been the Colonel Ducroix. Moved mechanically urged his hat and long whistling in. The Professor had fallen upon it was like a man who had been. The world is not serious. I am afraid of the Marquis off his spectacles that did not. Believe in his hand off the other things I say that he. Is not so much more valuable force right enough to. Step by the I do not a man who has left Sloane Square. They were only black cigar and brooding on the. World is an artist had fallen upon me the cry of. Cleaner things that looked at him like the Colonel talked to. Me to know what seemed to tell you in the man. In the President with a man who had led to a moment when he said Syme was. A low lights of the world he. Would always had the end I say that he had come to. \n",
      "Total words:  504\n",
      "was\n"
     ]
    }
   ],
   "source": [
    "def znajdz(ziarno):\n",
    "    for w in f1:\n",
    "        if w[0]==ziarno and w[1].isalpha():\n",
    "                if len(wynik)==0:\n",
    "                    wynik.append(ziarno.capitalize())\n",
    "                else:\n",
    "                    wynik.append(ziarno)\n",
    "                return w[1]\n",
    "    return \"I\"\n",
    "\n",
    "def znajdz2(ziarno):\n",
    "    lista_slow=[]\n",
    "    lista_slow.append(ziarno)\n",
    "    lista_slow+=get_all_word_synonyms(ziarno)\n",
    "    for z in lista_slow:\n",
    "        for w in f1:\n",
    "            if w[0]==z and w[1].isalpha():\n",
    "                if w[1] not in wynik[:1]:\n",
    "                    if len(wynik)==0:\n",
    "                        wynik.append(z.capitalize())\n",
    "                    else:\n",
    "                        wynik.append(z)\n",
    "                    return w[1]\n",
    "        return \"I\"\n",
    "\n",
    "def znajdz3(ziarno):\n",
    "    lista_slow=[]\n",
    "    lista_slow.append(ziarno)\n",
    "    lista_slow+=get_all_word_synonyms(ziarno)\n",
    "\n",
    "    znalezione_dopasowania=[]\n",
    "    for z in lista_slow:\n",
    "        for w in f2:\n",
    "            if w[0]==z and w[1].isalpha() and len(znalezione_dopasowania)<10:\n",
    "                znalezione_dopasowania.append(w)\n",
    "\n",
    "        #losujemy ze znalezionych\n",
    "        d1=len(znalezione_dopasowania)\n",
    "\n",
    "        if d1>0:\n",
    "            d2=randrange(0,d1)\n",
    "            w=znalezione_dopasowania[d2]\n",
    "            #dodajemy do wyniku i wybieramy ziarno\n",
    "            if w[0]==z and w[1].isalpha():\n",
    "                    if w[1] not in wynik[-30:]:\n",
    "                        if len(wynik)==0:\n",
    "                            wynik.append(z.capitalize())\n",
    "                            return w[1]\n",
    "                        else:\n",
    "                            if len(w)==3:\n",
    "                                if w[0].isalpha(): wynik.append(w[0])\n",
    "                                if w[1].isalpha(): wynik.append(w[1])\n",
    "                                if w[2].isalpha(): return w[2]\n",
    "                                else:\n",
    "                                    wynik.pop()\n",
    "                                    return w[1]\n",
    "\n",
    "                            wynik.append(z)\n",
    "            \n",
    "    \n",
    "    return \"I\"\n",
    "\n",
    "#generuje kilka zdań\n",
    "zdania = []\n",
    "wynik = []\n",
    "total_words=0\n",
    "word_that_cant_be_at_the_end=['DT','CC','IN','TO','IN','PRP']\n",
    "print(ziarno)\n",
    "\n",
    "while(len(zdania)<3 or total_words<int(max_slow)):\n",
    "    \n",
    "    if len(wynik)<randrange(6,30):\n",
    "        ziarno = znajdz3(ziarno)\n",
    "    else:\n",
    "        k=nltk.pos_tag(wynik[-1])[0][1]\n",
    "        while(k in word_that_cant_be_at_the_end):\n",
    "            wynik.pop()\n",
    "            k=nltk.pos_tag(wynik[-1])[0][1]\n",
    "        wynik[-1]+=\".\"\n",
    "        zdania.append(wynik)\n",
    "        \n",
    "        total_words+=len(wynik) #liczymy slowa\n",
    "        wynik=[]\n",
    "    \n",
    "s=\"\"\n",
    "for z in zdania:\n",
    "    for w in z:\n",
    "        s+=w+\" \"\n",
    "print(s)\n",
    "print(\"Total words: \", total_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 450,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "[]\n",
      "[('a', 'baboon', 'he'), ('a', 'baby', 's'), ('a', 'background', 'there'), ('a', 'backwoodsman', 'swings'), ('a', 'bad', 'cold'), ('a', 'bad', 'leak'), ('a', 'baked', 'brick'), ('a', 'bald', 'headed'), ('a', 'ball', 'as'), ('a', 'ball', 'of')]\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I\n"
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
